{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_string(text, stem=\"None\"):\n",
    "    \n",
    "    final_string = \"\"\n",
    "\n",
    "    # Make lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove line breaks\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "\n",
    "    # Remove puncuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "\n",
    "    # Stem or Lemmatize\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    elif stem == 'Spacy':\n",
    "        text_filtered = nlp(' '.join(text_filtered))\n",
    "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "\n",
    "    final_string = ' '.join(text_stemmed)\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitToList(df):\n",
    "    sentenceList = []\n",
    "    for index, row in df.iterrows():\n",
    "        tempList = []\n",
    "        tempRow = row['MDA'].split('.')\n",
    "        for x in tempRow:\n",
    "            if len(x) > 30 and not x[:5].isupper() and '#160' not in x:\n",
    "                tempList.append(clean_string(x, stem='Stem'))\n",
    "        sentenceList.append(tempList)\n",
    "    return sentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 19:01:05.947505: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-21 19:01:05.947660: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at yiyanghkust/finbert-tone.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "finbert = TFBertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "def modeling(df):\n",
    "    sentenceList = []\n",
    "    for index, row in df.iterrows():\n",
    "        tempList = []\n",
    "        tempRow = row['MDAList']\n",
    "        for x in tempRow:\n",
    "            sents = nlp(x)\n",
    "            tempList.append(sents)\n",
    "        sentenceList.append(tempList)\n",
    "    return sentenceList    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompanyTickers = [\n",
    "    # 'AAPl',\n",
    "    # 'TSLA', \n",
    "    # 'LULU',\n",
    "    # 'SQ',\n",
    "    # 'MSFT',\n",
    "    # 'GOOG', \n",
    "    # 'KO',\n",
    "    # 'XOM',\n",
    "    # 'MCD',\n",
    "    'GS'\n",
    "    # 'JPM'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/chrisjackson/LHL/Final Project/Initial Data/GS.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS split\n",
      "GS analyzed\n",
      "GS pickled\n"
     ]
    }
   ],
   "source": [
    "for ticker in CompanyTickers:\n",
    "    df = pd.read_pickle('/Users/chrisjackson/LHL/Final Project/Initial Data/GS.pkl')\n",
    "    df['MDAList'] = splitToList(df)\n",
    "    print(ticker, 'split')\n",
    "    df['analyzed'] = modeling(df)\n",
    "    print(ticker, 'analyzed')\n",
    "    open_file = open(ticker + 'Analyzed.pkl', \"wb\")\n",
    "    pickle.dump(df, open_file)\n",
    "    open_file.close()\n",
    "    print(ticker, 'pickled')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mini_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f75587dacee627cd4f922b679bcd65529678f042ef87d889d13fa7dbfc6775aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
